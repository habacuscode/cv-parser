{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "from datetime import date\n",
    "import pymupdf, os, json, ast\n",
    "import vertexai\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.vertex import Vertex\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from google.oauth2 import service_account\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, EmailStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder_path = 'data/cv'\n",
    "output_folder_path = 'data/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person(BaseModel):\n",
    "    first_name: Optional[str] = Field(description=\"First name of the person\")\n",
    "    last_name: Optional[str] = Field(description=\"Last name of the person\")\n",
    "    email: Optional[EmailStr] = Field(description=\"Email address of the person\")\n",
    "    phone: Optional[str] = Field(description=\"Contact phone number\")\n",
    "    address: str = Field(description=\"Residential address\")\n",
    "    city: Optional[str] = Field(description=\"City of residence\")\n",
    "    country: Optional[str] = Field(description=\"Country of residence\")\n",
    "    date_of_birth: Optional[str] = Field(None, description=\"Date of birth of the person\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkExperiences(BaseModel):\n",
    "    job_title: Optional[str] = Field(description=\"Title of the job role\")\n",
    "    employer: str = Field(description=\"Name of the employer or organization\")\n",
    "    start_date: Optional[str] = Field(description=\"Start date of the job\")\n",
    "    end_date: Optional[str] = Field(description=\"End date of the job, if applicable\")\n",
    "    city: Optional[str] = Field(description=\"City where the job was based\")\n",
    "    country: Optional[str] = Field(description=\"Country where the job was based\")\n",
    "    description: Optional[str] = Field(description=\"Description of the responsibilities and achievements in the job\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkExperiencesList(BaseModel):\n",
    "    experiences: Optional[List[WorkExperiences]] = Field(description=\"List of all working experiences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Educations(BaseModel):\n",
    "    \"\"\"\n",
    "    Information about academic background and education path (could include college/university, high school, specialization courses, etc.)\n",
    "    \"\"\"\n",
    "\n",
    "    degree: Optional[str] = Field(description=\"Degree or qualification obtained\")\n",
    "    field_of_study: Optional[str] = Field(description=\"Field of study or specialization\")\n",
    "    institution_name: Optional[str] = Field(description=\"Name of the educational institution\")\n",
    "    start_date: Optional[str] = Field(description=\"Start date of the educational program\")\n",
    "    end_date: Optional[str] = Field(description=\"End date of the educational program, if applicable\")\n",
    "    city: Optional[str] = Field(description=\"City where the institution is located\")\n",
    "    country: Optional[str] = Field(description=\"Country where the institution is located\")\n",
    "    description: Optional[str] = Field(description=\"Additional details about the education\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EducationsList(BaseModel):\n",
    "    \"\"\"\n",
    "    List of Educations Pydantic objects\n",
    "    \"\"\"\n",
    "    experiences: Optional[List[Educations]] = Field(description=\"List of academic education experiences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonSkills(BaseModel):\n",
    "    skill_name: Optional[str] = Field(description=\"Name of the skill\")\n",
    "    skilltype: Optional[str] = Field(description=\"Type of skill, e.g., technical or soft skill\")\n",
    "    proficiency_level: Optional[str] = Field(description=\"Proficiency level in the skill, e.g., beginner, intermediate, advanced\")\n",
    "    years_of_experience: Optional[int] = Field(None, description=\"Years of experience with the skill\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonSkillList(BaseModel):\n",
    "    experiences: List[PersonSkills] = Field(description=\"List of all skills of the person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonLanguages(BaseModel):\n",
    "    language_name: Optional[str] = Field(description=\"Language name\")\n",
    "    proficiency_level: Optional[str] = Field(description=\"Proficiency level in the language, e.g., native, fluent, intermediate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonLanguagesList(BaseModel):\n",
    "    experiences: List[PersonLanguages] = Field(description=\"List of all languages spoken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Certifications(BaseModel):\n",
    "    name: Optional[str] = Field(description=\"Name of the certification\")\n",
    "    issuing_body: Optional[str] = Field(description=\"Organization that issued the certification\")\n",
    "    issue_date: Optional[str] = Field(description=\"Date the certification was issued\")\n",
    "    expiration_date: Optional[str] = Field(description=\"Expiration date of the certification, if applicable\")\n",
    "    description: Optional[str] = Field(description=\"Additional details about the certification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CertificationList(BaseModel):\n",
    "    experiences: List[Certifications] = Field(description=\"List of all certifications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projects(BaseModel):\n",
    "    name: Optional[str] = Field(description=\"Name of the project\")\n",
    "    description: Optional[str] = Field(None, description=\"Details about the project\")\n",
    "    start_date: Optional[str] = Field(description=\"Start date of the project\")\n",
    "    end_date: Optional[str] = Field(None, description=\"End date of the project, if applicable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectsList(BaseModel):\n",
    "    experiences: List[Projects] = Field(description=\"List of all projects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = pymupdf.open(os.path.join(input_folder_path, 'Alessandra_Saitta_ITA.pdf')) # open a document\n",
    "out = open(os.path.join(output_folder_path, \"Alessandra_Saitta_ITA.txt\"), \"wb\") # create a text output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in doc: # iterate the document pages\n",
    "    text = page.get_text().encode(\"utf8\") # get plain text (is in UTF-8)\n",
    "    out.write(text) # write text of page\n",
    "    out.write(bytes((12,))) # write page delimiter (form feed 0x0C)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_folder_path, \"Alessandra_Saitta_ITA.txt\"), \"rb\") as file:  # open the file in binary read mode\n",
    "    cv_text = file.read().decode('utf-8')  # read the content of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')\n",
    "credentials: service_account.Credentials = (\n",
    "    service_account.Credentials.from_service_account_file(filename)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_source_models = ['qwen2:7b', 'llama3.2:3b', 'llama3.1:latest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-4o\")\n",
    "llm_open = Ollama(model=open_source_models[1], request_timeout=120.0)\n",
    "llm_gemini = Vertex(model=\"gemini-1.5-pro\", project=credentials.project_id, credentials=credentials, location=\"europe-west4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    \"\"\"\n",
    "    I will provide an input text containing unstructured data from a person’s resume. \n",
    "    This data may include details about work experience, personal information, academic background, skills, certifications, and more.\n",
    "\n",
    "    Key considerations:\n",
    "\n",
    "    - You will likely get personal information which are very detailed, you can likely parse all the information needed.\n",
    "    - Information about city and country may be embedded in the address field; extract it from there if applicable.\n",
    "    - When the expected data type is a list, you should output a list which has several Pydantic DTO elements in it. It should not be a string.\n",
    "    - Info about work and academic experiences may contain multiple elements. Pay close attention to the requested data type and don't try to force it.\n",
    "    - If any requested data is unavailable in the input, leave the corresponding fields with an empty string without making assumptions.\n",
    "    - Be mindful that certain sections, like work experience and academic background, may include multiple entries.\n",
    "    \n",
    "    Your task is to parse this data into a structured format, accurately reflecting all the available information.\n",
    "\n",
    "    {text}\n",
    "\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json_response(dto, source_llm: str = 'google'):\n",
    "    \n",
    "    if source_llm == 'ollama':\n",
    "        response = llm_open.structured_predict(\n",
    "            dto, prompt, text=cv_text\n",
    "            )\n",
    "        print(response)\n",
    "        # json_output = response.model_dump_json()\n",
    "        json_output = ast.literal_eval(response).model_dump_json()\n",
    "    elif source_llm == 'google':\n",
    "        response = llm_gemini.structured_predict(\n",
    "            dto, prompt, text=cv_text\n",
    "            )\n",
    "        json_output = response.model_dump_json()\n",
    "    else:\n",
    "        response = llm.structured_predict(\n",
    "            dto, prompt, text=cv_text\n",
    "            )\n",
    "        json_output = response.model_dump_json()\n",
    "    return json.dumps(json.loads(json_output), indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_name='Alessandra' last_name='Saitta' email='alessandra.saitta93@gmail.com' phone='+39 3335462586' address='Via Cappellini, 7, 21049, Tradate, Italia (Abitazione)' city='Tradate' country='Italia' date_of_birth='26/04/1993'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "malformed node or string: Person(first_name='Alessandra', last_name='Saitta', email='alessandra.saitta93@gmail.com', phone='+39 3335462586', address='Via Cappellini, 7, 21049, Tradate, Italia (Abitazione)', city='Tradate', country='Italia', date_of_birth='26/04/1993')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m person \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_json_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPerson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_llm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mollama\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(person)\n",
      "Cell \u001b[0;32mIn[70], line 9\u001b[0m, in \u001b[0;36mcreate_json_response\u001b[0;34m(dto, source_llm)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# json_output = response.model_dump_json()\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     json_output \u001b[38;5;241m=\u001b[39m \u001b[43mast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mliteral_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmodel_dump_json()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m source_llm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoogle\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     11\u001b[0m     response \u001b[38;5;241m=\u001b[39m llm_gemini\u001b[38;5;241m.\u001b[39mstructured_predict(\n\u001b[1;32m     12\u001b[0m         dto, prompt, text\u001b[38;5;241m=\u001b[39mcv_text\n\u001b[1;32m     13\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-macos-aarch64-none/lib/python3.11/ast.py:110\u001b[0m, in \u001b[0;36mliteral_eval\u001b[0;34m(node_or_string)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m left \u001b[38;5;241m-\u001b[39m right\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert_signed_num(node)\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_or_string\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-macos-aarch64-none/lib/python3.11/ast.py:109\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m left \u001b[38;5;241m-\u001b[39m right\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_signed_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-macos-aarch64-none/lib/python3.11/ast.py:83\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert_signed_num\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m operand\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-macos-aarch64-none/lib/python3.11/ast.py:74\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert_num\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_num\u001b[39m(node):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, Constant) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(node\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mcomplex\u001b[39m):\n\u001b[0;32m---> 74\u001b[0m         \u001b[43m_raise_malformed_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-macos-aarch64-none/lib/python3.11/ast.py:71\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._raise_malformed_node\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lno \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(node, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlineno\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     70\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlno\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: malformed node or string: Person(first_name='Alessandra', last_name='Saitta', email='alessandra.saitta93@gmail.com', phone='+39 3335462586', address='Via Cappellini, 7, 21049, Tradate, Italia (Abitazione)', city='Tradate', country='Italia', date_of_birth='26/04/1993')"
     ]
    }
   ],
   "source": [
    "person = create_json_response(Person, source_llm='ollama')\n",
    "print(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 validation error for WorkExperiencesList\n",
      "experiences\n",
      "  Input should be a valid list [type=list_type, input_value=\"[{'start_date': '04/07/2...on': '', 'skills': []}]\", input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/list_type\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<unknown>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/Desktop/habacus/cv-parser/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[73], line 1\u001b[0m\n    work_experiences = create_json_response(WorkExperiencesList, source_llm='ollama')\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[70], line 9\u001b[0m in \u001b[1;35mcreate_json_response\u001b[0m\n    json_output = ast.literal_eval(response).model_dump_json()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/.local/share/uv/python/cpython-3.11.10-macos-aarch64-none/lib/python3.11/ast.py:64\u001b[0m in \u001b[1;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string.lstrip(\" \\t\"), mode='eval')\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-macos-aarch64-none/lib/python3.11/ast.py:50\u001b[0;36m in \u001b[0;35mparse\u001b[0;36m\n\u001b[0;31m    return compile(source, filename, mode, flags,\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<unknown>:1\u001b[0;36m\u001b[0m\n\u001b[0;31m    1 validation error for WorkExperiencesList\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "work_experiences = create_json_response(WorkExperiencesList, source_llm='ollama')\n",
    "print(work_experiences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 validation error for PersonLanguagesList\n",
      "experiences\n",
      "  Input should be a valid list [type=list_type, input_value='[{\"city\": \"Milano\", \"cou...oogle Cloud Platform\"}]', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/list_type\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'model_dump_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m person_languages \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_json_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPersonLanguagesList\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_llm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mollama\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(person_languages)\n",
      "Cell \u001b[0;32mIn[62], line 8\u001b[0m, in \u001b[0;36mcreate_json_response\u001b[0;34m(dto, source_llm)\u001b[0m\n\u001b[1;32m      4\u001b[0m     response \u001b[38;5;241m=\u001b[39m llm_open\u001b[38;5;241m.\u001b[39mstructured_predict(\n\u001b[1;32m      5\u001b[0m         dto, prompt, text\u001b[38;5;241m=\u001b[39mcv_text\n\u001b[1;32m      6\u001b[0m         )\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[0;32m----> 8\u001b[0m     json_output \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump_json\u001b[49m()\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# json_output = ast.literal_eval(response).model_dump_json()\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m source_llm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoogle\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'model_dump_json'"
     ]
    }
   ],
   "source": [
    "person_languages = create_json_response(PersonLanguagesList, source_llm='ollama')\n",
    "print(person_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": 1,\n",
      "  \"name\": \"Python\",\n",
      "  \"type\": \"technical\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "skill_json_output = skill_response.model_dump_json()\n",
    "print(json.dumps(json.loads(skill_json_output), indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": 1,\n",
      "  \"person_id\": 1,\n",
      "  \"skill_id\": 1,\n",
      "  \"proficiency_level\": \"advanced\",\n",
      "  \"years_of_experience\": 3\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "person_skill_json_output = person_skill_response.model_dump_json()\n",
    "print(json.dumps(json.loads(person_skill_json_output), indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project=credentials.project_id, location=\"europe-west4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatVertexAI(model=\"gemini-1.5-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text =  \"\"\"\n",
    "    I will provide an input text containing unstructured data from a person’s resume. \n",
    "    This data may include details about work experience, personal information, academic background, skills, certifications, and more.\n",
    "\n",
    "    Key considerations:\n",
    "\n",
    "    - You will likely get personal information which are very detailed, you can likely parse all the information needed.\n",
    "    - Information about city and country may be embedded in the address field; extract it from there if applicable.\n",
    "    - When the expected data type is a list, you should output a list which has several Pydantic DTO elements in it. It should not be a string.\n",
    "    - Info about work and academic experiences may contain multiple elements. Pay close attention to the requested data type and don't try to force it.\n",
    "    - If any requested data is unavailable in the input, leave the corresponding fields with an empty string without making assumptions.\n",
    "    - Be mindful that certain sections, like work experience and academic background, may include multiple entries.\n",
    "    \n",
    "    Your task is to parse this data into a structured format, accurately reflecting all the available information.\n",
    "\n",
    "    Wrap the output in `json` tags\\n{format_instructions}\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text_list =  \"\"\"\n",
    "    I will provide an input text containing unstructured data from a person’s resume. \n",
    "    This data may include details about work experience, personal information, academic background, skills, certifications, and more.\n",
    "\n",
    "    Key considerations:\n",
    "\n",
    "    - You will likely get personal information which are very detailed, you can likely parse all the information needed.\n",
    "    - Information about city and country may be embedded in the address field; extract it from there if applicable.\n",
    "    - You should output a list which has several Pydantic DTO elements in it. It should not be a string or a list with a single element, except when \n",
    "    the person had just one single experience\n",
    "    - Info about work and academic experiences may contain multiple elements.\n",
    "    - If any requested data is unavailable in the input, leave the corresponding fields with an empty string without making assumptions.\n",
    "    - Be mindful that certain sections, like work experience and academic background, may include multiple entries.\n",
    "    \n",
    "    Your task is to parse this data into a structured format, accurately reflecting all the available information.\n",
    "\n",
    "    Wrap the output in `json` tags\\n{format_instructions}\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person(BaseModel):\n",
    "    first_name: str = Field(description=\"First name of the person\")\n",
    "    last_name: str = Field(description=\"Last name of the person\")\n",
    "    email: EmailStr = Field(description=\"Email address of the person\")\n",
    "    phone: str = Field(description=\"Contact phone number\")\n",
    "    address: str = Field(description=\"Residential address\")\n",
    "    city: str = Field(description=\"City of residence\")\n",
    "    country: str = Field(description=\"Country of residence\")\n",
    "    date_of_birth: str = Field(None, description=\"Date of birth of the person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a parser\n",
    "parser = PydanticOutputParser(pydantic_object=EducationsList)\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            prompt_text_list,\n",
    "        ),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ").partial(format_instructions=parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: \n",
      "    I will provide an input text containing unstructured data from a person’s resume. \n",
      "    This data may include details about work experience, personal information, academic background, skills, certifications, and more.\n",
      "\n",
      "    Key considerations:\n",
      "\n",
      "    - You will likely get personal information which are very detailed, you can likely parse all the information needed.\n",
      "    - Information about city and country may be embedded in the address field; extract it from there if applicable.\n",
      "    - You should output a list which has several Pydantic DTO elements in it. It should not be a string or a list with a single element, except when \n",
      "    the person had just one single experience\n",
      "    - Info about work and academic experiences may contain multiple elements.\n",
      "    - If any requested data is unavailable in the input, leave the corresponding fields with an empty string without making assumptions.\n",
      "    - Be mindful that certain sections, like work experience and academic background, may include multiple entries.\n",
      "    \n",
      "    Your task is to parse this data into a structured format, accurately reflecting all the available information.\n",
      "\n",
      "    Wrap the output in `json` tags\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"$defs\": {\"Educations\": {\"properties\": {\"degree\": {\"description\": \"Degree or qualification obtained\", \"title\": \"Degree\", \"type\": \"string\"}, \"field_of_study\": {\"description\": \"Field of study or specialization\", \"title\": \"Field Of Study\", \"type\": \"string\"}, \"institution_name\": {\"description\": \"Name of the educational institution\", \"title\": \"Institution Name\", \"type\": \"string\"}, \"start_date\": {\"description\": \"Start date of the educational program\", \"title\": \"Start Date\", \"type\": \"string\"}, \"end_date\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"default\": null, \"description\": \"End date of the educational program, if applicable\", \"title\": \"End Date\"}, \"city\": {\"description\": \"City where the institution is located\", \"title\": \"City\", \"type\": \"string\"}, \"country\": {\"description\": \"Country where the institution is located\", \"title\": \"Country\", \"type\": \"string\"}, \"description\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"default\": null, \"description\": \"Additional details about the education\", \"title\": \"Description\"}}, \"required\": [\"degree\", \"field_of_study\", \"institution_name\", \"start_date\", \"city\", \"country\"], \"title\": \"Educations\", \"type\": \"object\"}}, \"properties\": {\"experiences\": {\"anyOf\": [{\"items\": {\"$ref\": \"#/$defs/Educations\"}, \"type\": \"array\"}, {\"type\": \"null\"}], \"description\": \"List of academic education experiences\", \"title\": \"Experiences\"}}, \"required\": [\"experiences\"]}\n",
      "```\n",
      "\n",
      "    \n",
      "Human: Alessandra Saitta \n",
      "Data di nascita: 26/04/1993\n",
      " Nazionalità: Italiana\n",
      " Numero di telefono: (+39) 3335462586 (Cellulare) \n",
      " \n",
      "Indirizzo e-mail: alessandra.saitta93@gmail.com \n",
      " Indirizzo: Via Cappellini, 7, 21049, Tradate, Italia (Abitazione) \n",
      " \n",
      "04/07/2021 – 04/07/2024 Milano, Italia \n",
      "PYTHON DEVELOPER & DATA SCIENTIST AMIKO S.R.L. \n",
      "Sono stata parte del team di Ricerca e Sviluppo di Amiko, azienda specializzata nell'applicazione di Intelligenza\n",
      "artificiale alla medicina respiratoria. In particolare le mie mansioni consistevano in:\n",
      "• Sviluppo di software di back-office software per la raccolta, analisi e organizzazione di dati medici provenienti dalla\n",
      "piattaforma aziendale. Linguaggi utilizzati: Python e QML.\n",
      "• Progettazione di modelli di apprendimento automatico(machine learning) applicati a dati clinici. Linguaggio utilizzato:\n",
      "Python\n",
      "• Collaborazione con team multidisciplinari per garantire l'integrazione e l'efficacia delle soluzioni di intelligenza\n",
      "artificiale all'interno dei processi aziendali.\n",
      "04/2020 – 04/2021 Milano , Italia \n",
      "DATA SCIENTIST & DATA ENGINEER UNITED RISK S.P.A \n",
      "Come Data Scientist ho costruito un modello per la classificazione delle aree inquinate in Italia. In particolare, i miei\n",
      "compiti includevano:\n",
      "• Recupero e pulizia dei dati da fonti eterogenee,\n",
      "• Web scraping,\n",
      "• Clustering delle aree geografiche di interesse,\n",
      "• Costruzione del dataset,\n",
      "• Selezione delle caratteristiche,\n",
      "• Applicazione e ottimizzazione degli iperparametri degli algoritmi di classificazione,\n",
      "• Costruzione di reti neurali.\n",
      "Gli strumenti principali che ho utilizzato sono: Python, Jupyter, Pandas, Scikit-Learn, Keras.\n",
      "Da ottobre 2020 a febbraio 2021, ho realizzato un sistema basato su Google Cloud Platform per la generazione\n",
      "automatica di report mensili per i cantieri. Ho utilizzato Google Cloud Functions, BigQuery e Google Cloud Scheduler\n",
      "per recuperare e memorizzare dati da fonti eterogenee. Google Data Studio è stato adottato per la visualizzazione dei\n",
      "dati.\n",
      "10/2017 – 12/2018 Milano, Italia \n",
      "SVILUPPATORE SOFTWARE SOCIAL THINGUM S.R.L. \n",
      "• Sviluppatore junior back-end. Ho lavorato come sviluppatore Java sul lato server di un progetto IoT.\n",
      "• Sviluppatore junior full stack. Ho lavorato su un'applicazione web GIS. Le principali tecnologie che ho utilizzato per\n",
      "questo progetto sono MySQL, Java, Angular, Typescript, CSS, HTML e Git.\n",
      "09/2018 – 22/04/2021 Milano, Italia \n",
      "LAUREA MAGISTRALE IN INFORMATICA Università degli Studi di Milano \n",
      "Specializzazione: Data Science e Perceptual Computing\n",
      "Esami: Algorithms for Massive Datasets, Natural Interaction, Methods\n",
      "for Affective Computing, Bioinformatics, Information Management,\n",
      "Didactics for Computer Science, Intelligent Systems, Methods\n",
      "for Image Processing, Web Algorithmics, Information Retrieval,\n",
      "Statistical Methods for Machine Learning, Audio Pattern Recognition,\n",
      "ESPERIENZA LAVORATIVA\n",
      "ISTRUZIONE E FORMAZIONE\n",
      "\fMultimedial Organization and Digitalization.\n",
      "Sito Internet https://www.unimi.it/it/corsi/laurea-magistrale/informatica-magistrale \n",
      "Voto finale 104/110 \n",
      "Livello EQF Livello 7 EQF \n",
      "Tesi Applicazione di algoritmi di apprendimento per la riqualificazione delle aree dismesse nelle regioni italiane \n",
      "Milano, Italia \n",
      "LAUREA TRIENNALE IN INFORMATICA Università degli Studi di Milano \n",
      "Sito Internet https://www.unimi.it/ \n",
      "Livello EQF Livello 6 EQF \n",
      "Tesi Tecniche di estrazione di modelli 3D del volto da video \n",
      "Lingua madre:  ITALIANO \n",
      "Altre lingue:\n",
      "  \n",
      "COMPRENSIONE\n",
      "ESPRESSIONE ORALE\n",
      "SCRITTURA\n",
      "Ascolto\n",
      "Lettura\n",
      "Produzione orale\n",
      "Interazione orale\n",
      "INGLESE \n",
      "C1\n",
      "C2\n",
      "C1\n",
      "C1\n",
      "B2\n",
      "Livelli: A1 e A2: Livello elementare B1 e B2: Livello intermedio C1 e C2: Livello avanzato \n",
      "Conoscenza avanzata di Python e delle principali librerie (NumPy, Pandas, scikit-learn, Matplotlib)\n",
      " Programmazione:\n",
      "Python, C, Java, Bash, LaTex, Git\n",
      " Gestione database(Linguaggio SQL)\n",
      " QML\n",
      " Google Cloud Platform (BigQuery,\n",
      "Storage) \n",
      "2021 \n",
      "Language-agnostic speech anger identification \n",
      "Autorizzo il trattamento dei miei dati personali presenti nel CV ai sensi dell’art. 13 d. lgs. 30 giugno 2003 n. 196 - “Codice in\n",
      "materia di protezione dei dati personali” e dell’art. 13 GDPR 679/16 - “Regolamento europeo sulla protezione dei dati personali”. \n",
      "COMPETENZE LINGUISTICHE \n",
      "COMPETENZE DIGITALI \n",
      "PUBBLICAZIONI \n",
      "\f\n"
     ]
    }
   ],
   "source": [
    "print(prompt.invoke({\"text\": cv_text}).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EducationsList(experiences=[Educations(degree='LAUREA MAGISTRALE IN INFORMATICA', field_of_study='Data Science e Perceptual Computing', institution_name='Università degli Studi di Milano', start_date='09/2018', end_date='22/04/2021', city='Milano', country='Italia', description='Esami: Algorithms for Massive Datasets, Natural Interaction, Methods\\nfor Affective Computing, Bioinformatics, Information Management,\\nDidactics for Computer Science, Intelligent Systems, Methods\\nfor Image Processing, Web Algorithmics, Information Retrieval,\\nStatistical Methods for Machine Learning, Audio Pattern Recognition,\\nMultimedial Organization and Digitalization.\\nSito Internet https://www.unimi.it/it/corsi/laurea-magistrale/informatica-magistrale \\nVoto finale 104/110 \\nLivello EQF Livello 7 EQF \\nTesi Applicazione di algoritmi di apprendimento per la riqualificazione delle aree dismesse nelle regioni italiane'), Educations(degree='LAUREA TRIENNALE IN INFORMATICA', field_of_study=None, institution_name='Università degli Studi di Milano', start_date=None, end_date=None, city='Milano', country='Italia', description='Sito Internet https://www.unimi.it/ \\nLivello EQF Livello 6 EQF \\nTesi Tecniche di estrazione di modelli 3D del volto da video')])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | parser\n",
    "\n",
    "chain.invoke({\"text\": cv_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
